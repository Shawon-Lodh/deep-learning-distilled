# deep-learning-distilled
Notes on some important deep learning topics and paper summaries.</br>
Feel free to contribute.</br>
Documents have corresponding latex file you can edit.

### Notes added so far
* **Information, Entropy, Cross-Entropy: ML perspective:** Basic of information theory, why *log* is used to represent information. entropy, cross entropy, KL divergence, likelihood, why cross entropy loss is used in machine learning. 
* **Gradient Descent Optimizations:** Three gradient descent varients, chalenges with vanilla gradient descent, momentum, Nesterov accelerated gradient, Adagrad, RMSprop, Adadelta, Adam.
* **Common activation functions used in neural net:** why we need to use activation function, desirable properties of activation, sigmoid, tanh, relu, prelu, elu.
* **[Why ReLU(instead of sigmoid/tanh)?:](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/notes/Why%20ReLU.pdf)** why ReLU is better suited for deep learning compared to sigmoid or tanh, some of the potential problems with ReLU and how to mitigate them.

### Paper Summary
1. [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)
2. [Learning and transferring mid-Level image representations using convolutional neural networks](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)
3. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
4. [Sequence to Sequence Learning with Neural Networks](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf)
5. [Distributed Representations of Sentences and Documents](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Distributed%20Representations%20of%20Sentences%20and%20Documents.pdf)
6. [VGG](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition(VGG).pdf)
7. [ResNet](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf)
8. [Deep Sparse Rectifier Neural Networks](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Deep%20Sparse%20Rectifier%20Neural%20Networks.pdf)
9. [Network in Network](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Network%20in%20Network.pdf)
10. [GoogLeNet](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Going%20deeper%20with%20convolutions.pdf)
11. [MobileNets](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/MobileNets-Efficient%20Convolutional%20Neural%20Networks%20for%20Mobile%20Vision%20Applications.pdf)
12. [AlexNet](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks.pdf)

### Notes to add
- [ ] Regularization
- [ ] Different types of Losses
- [ ] Activation functions: swish
