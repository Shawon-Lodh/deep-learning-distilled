# deep-learning-distilled
Notes on some important deep learning topics and paper summaries.</br>
Feel free to contribute.</br>
Accepting latex conversion of the documents.

### Notes added so far
* **Information, Entropy, Cross-Entropy: ML perspective:** Basic of information theory, why *log* is used to represent information. entropy, cross entropy, KL divergence, likelihood, why cross entropy loss is used in machine learning. 
* **Gradient Descent Optimizations:** Three gradient descent varients, chalenges with vanilla gradient descent, momentum, Nesterov accelerated gradient, Adagrad, RMSprop, Adadelta, Adam.

### Paper Summary
1. How transferable are features in deep neural networks? [https://arxiv.org/abs/1411.1792]

### Notes to add
- [ ] Regularization
- [ ] Different types of Losses
- [ ] Activation functions
