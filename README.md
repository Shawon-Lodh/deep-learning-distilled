# deep-learning-distilled
Notes on some important deep learning topics and paper summaries.</br>
Feel free to contribute.</br>
Accepting latex conversion of the documents.

### Notes added so far
* **Information, Entropy, Cross-Entropy: ML perspective:** Basic of information theory, why *log* is used to represent information. entropy, cross entropy, KL divergence, likelihood, why cross entropy loss is used in machine learning. 
* **Gradient Descent Optimizations:** Three gradient descent varients, chalenges with vanilla gradient descent, momentum, Nesterov accelerated gradient, Adagrad, RMSprop, Adadelta, Adam.
* **Common activation functions used in neural net:** why we need to use activation function, desirable properties of activation, sigmoid, tanh, relu, prelu, elu.

### Paper Summary
1. [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)
2. [Learning and transferring mid-Level image representations using convolutional neural networks](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)
3. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
4. [Sequence to Sequence Learning with Neural Networks]https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf)
5. [Distributed Representations of Sentences and Documents](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Distributed%20Representations%20of%20Sentences%20and%20Documents.pdf)


### Notes to add
- [ ] Regularization
- [ ] Different types of Losses
- [ ] Activation functions: swish
