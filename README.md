# deep-learning-distilled
Notes on some important deep learning topics and paper summaries.</br>
Feel free to contribute.</br>
Accepting latex conversion of the documents.

### Notes added so far
* **Information, Entropy, Cross-Entropy: ML perspective:** Basic of information theory, why *log* is used to represent information. entropy, cross entropy, KL divergence, likelihood, why cross entropy loss is used in machine learning. 
* **Gradient Descent Optimizations:** Three gradient descent varients, chalenges with vanilla gradient descent, momentum, Nesterov accelerated gradient, Adagrad, RMSprop, Adadelta, Adam.
* **Common activation functions used in neural net:** why we need to use activation function, desirable properties of activation, sigmoid, tanh, relu, prelu, elu.

### Paper Summary
1. [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)
2. [Learning and transferring mid-Level image representations using convolutional neural networks](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)


### Notes to add
- [ ] Regularization
- [ ] Different types of Losses
- [ ] Activation functions
