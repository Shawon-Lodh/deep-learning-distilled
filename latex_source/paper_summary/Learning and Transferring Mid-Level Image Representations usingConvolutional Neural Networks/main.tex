\documentclass{article}
\usepackage[utf8]{inputenc}


\title{Learning and Transferring Mid-Level Image Representations
using Convolutional Neural Networks}
\author{}
\date{}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage[left=2.5cm,right=2.5cm,top=1cm,bottom=1.25cm]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue}



\begin{document}

\maketitle

\section*{Link}
\href{https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf}{pdf} 


\section*{Summary}
\begin{itemize}
    \item Training large CNN models require lots of training data which are not always available for the task at hand. To address this issue we can train a model on large scale image dataset and transfer the learned image representation to a task with limited amount of data. Here ImageNet is used for training a deep CNN model whose features are then transferred to Pascal VOC dataset.
    \item When transferring image features we need to be aware of following issues that might arise
    \begin{enumerate}
        \item \textit{Label bias}: Labels(class) is source task may be very different from labels in the target task.
        \item \textit{Dataset capture bias}: The distributions of object orientations, sizes, mutual exclusion patterns etc. may be very different between the two tasks.
        \item \textit{Negative data bias}: The target task may contain many other objects in the background that are not present in the source task.
    \end{enumerate}
    \item To address label bias we design an architecture that remaps the class labels between source and target task. To achieve this we remove the output layer of the pre-trained network and add an adaptation layer formed by two fully connected layers. The transferred layers are kept fixed (frozen) when training on the target task and only adaptation layer is trained.
    \item To address dataset bias we employ a sliding window strategy and extract around 500 square patches from each image for Pascal VOC dataset by sampling eight different scales on a regularly spaced grid with at least 50\% overlap between neighboring patches.
    \item During test time we apply the network on each of the approximately 500 overlapping multi-scale patches and compute the overall contributions for each class. 
    \item There was a 8\% performance drop when training the identical network on Pascal VOC dataset from scratch(no pretrain) which indicates benefit of feature transfer. 
    \item Overlap between classes in the source and target domains may have a positive effect on the transfer.  
\end{itemize}

\end{document}
